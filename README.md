# Impact of Simulacra Roles on Large Language Model Responses: Does it Make a Difference?

This project aims to evaluate the impact of assigning persona-based roles to Large Language Models (LLMs) on their performance in various Natural Language Processing (NLP) tasks. The tasks include closed-ended questions (HellaSwag dataset) and summarization (CNN/DailyMail dataset).

## Table of Contents

- [Impact of Simulacra Roles on Large Language Model Responses: Does it Make a Difference?](#impact-of-simulacra-roles-on-large-language-model-responses-does-it-make-a-difference)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Requirements](#requirements)
  - [Usage](#usage)
  - [Datasets](#datasets)
  - [Results](#results)
  - [License](#license)

## Introduction

The use of persona-based roles in LLM prompts has become increasingly common, with the assumption that assigning specific roles to the model can enhance its performance on various tasks. However, this project seeks to critically examine this assumption and determine whether the assignment of persona-based roles significantly impacts the performance of LLMs.

The project evaluates three categories of LLMs:

1. DPO-tuned (Truthful)
2. Instruction-tuned (Mixtral)
3. Base/foundational (Gemma)

The experiments are conducted using a standardized prompting methodology to specify roles, eliminating potential bias stemming from variations in prompt wording.

## Requirements

- Python 3.10
- PyTorch
- vllm
- ujson
- tqdm

## Usage

1. Clone the repository

2. Install the required dependencies:

```bash
pip install -r requirements.txt
```

3. Prepare the input folders containing the JSONL files for evaluation.

4. Run the evaluation script:

```bash
python vllm-inference-v2.py input_folder1 input_folder2 ...
```

Replace `input_folder1`, `input_folder2`, etc., with the paths to your input folders containing the JSONL files.

The script will process the JSONL files in the specified input folders, generate outputs using the LLMs, and save the results in corresponding output folders.

## Datasets

The project uses two datasets for evaluation:

1. **HellaSwag**: A dataset for closed-ended questions, where the model is tasked with selecting the correct phrase excerpt to complete a given sentence.
2. **CNN/DailyMail**: A dataset for summarization, where the model is tasked with generating a short summary (2-3 sentences) of a given article.

You can find the input datasets in the lrztar-zipped folder `datasets.tar.lrz`. Download it from [here](https://evilscript.eu/upload/files/datasets.tar.lrz). Install [lrzip](https://github.com/ckolivas/lrzip) and extract the contents of the folder to access the datasets for evaluation.

## Results

The evaluation results demonstrate that the performance of LLMs is not significantly affected by the assignment of persona-based roles for generative NLP tasks. The findings are consistent across different categories of LLMs (DPO-tuned, Instruction-tuned, and base/foundational) and model sizes.

Statistical tests (ANOVA and Kruskal-Wallis) confirm that the introduction of different personas does not lead to systematically significant differences in the performance of LLMs.

You can find the results of the evaluation in the `results.tar.lrz` lrztar-zipped folder, which contains the output files generated by the evaluation script. It's a very large file, so you may need at least 20GB of disk space and 16GB of RAM to extract the contents. Download it from [here](https://evilscript.eu/upload/files/results.tar.lrz).

## License

This project is licensed under the [GNU GENERAL PUBLIC LICENSE](LICENSE).
